{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install psycopg2-binary pandas requests msal Office365-REST-Python-Client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mln3ja2CQg3N",
        "outputId": "93fb2e32-d567-42e0-ff63-7a82590c0f14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting msal\n",
            "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting Office365-REST-Python-Client\n",
            "  Downloading Office365_REST_Python_Client-2.6.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal) (2.10.1)\n",
            "Requirement already satisfied: cryptography<49,>=2.5 in /usr/local/lib/python3.12/dist-packages (from msal) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from Office365-REST-Python-Client) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<49,>=2.5->msal) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<49,>=2.5->msal) (2.23)\n",
            "Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Office365_REST_Python_Client-2.6.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary, msal, Office365-REST-Python-Client\n",
            "Successfully installed Office365-REST-Python-Client-2.6.2 msal-1.34.0 psycopg2-binary-2.9.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from office365.sharepoint.client_context import ClientContext\n",
        "from office365.runtime.auth.client_credential import ClientCredential\n",
        "from sqlalchemy import create_engine, MetaData, Table, insert\n",
        "from sqlalchemy.dialects.postgresql import insert\n",
        "import pandas as pd\n",
        "import io, json, os\n",
        "import msal\n",
        "import requests\n",
        "from pyspark.sql.functions import to_date\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "site_url = \"https://pivoxlabs1.sharepoint.com\"\n",
        "tenant_id = \"404ce9c5-4d66-49f1-a51b-6434c21a8e23\"\n",
        "client_id = \"66604cfa-0ed6-4882-ba7b-bb3ffe98ea5c\"\n",
        "client_secret = \"oMg8Q~ZeDls2hAgSL8ouB.5PPd0QKTtKC2nRFcsX\"\n",
        "authority = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
        "scope = [\"https://graph.microsoft.com/.default\"]\n",
        "# Environment variables\n",
        "host = \"docker.pivoxlabs.com\"\n",
        "port = 5432\n",
        "user = \"postgres\"\n",
        "password = \"postgres\"\n",
        "database = \"transactiondata\"\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "try:\n",
        "    engine = create_engine(f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}',pool_pre_ping=True)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to the database: {e}\")\n",
        "\n",
        "# df_pandas = df_spark.toPandas()  # df_spark is undefined\n",
        "metadata = MetaData()\n",
        "\n",
        "\n",
        "app = msal.ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
        "token = app.acquire_token_for_client(scopes=scope)\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {token['access_token']}\"}\n",
        "\n",
        "# Get files in the SharePoint folder via Graph\n",
        "graph_url = \"https://graph.microsoft.com/v1.0/sites/c8773cee-a56a-46ce-be78-889cfcac7255/drives/b!7jx3yGqlzka-eIic_KxyVdiiw1RTESFGoPwDKkw6zNI5hWQ6XmdBQIOlZNePPg2z/items/01ZR6EH4AXCJRV4EBCVFDLVILL3H7BRSWN/children\"\n",
        "res = requests.get(graph_url, headers=headers)\n",
        "res.raise_for_status()\n",
        "\n",
        "files = res.json()[\"value\"]\n",
        "\n",
        "output_dir = \"/Workspace/processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "combined_data = []  # list to collect all Excel rows\n",
        "\n",
        "for f in files:\n",
        "    graph_patch = \"https://graph.microsoft.com/v1.0/sites/c8773cee-a56a-46ce-be78-889cfcac7255/drives/b!7jx3yGqlzka-eIic_KxyVdiiw1RTESFGoPwDKkw6zNI5hWQ6XmdBQIOlZNePPg2z/items/\" + f[\"id\"]\n",
        "    if f[\"name\"].endswith(\".xlsx\"):\n",
        "        download_url = f[\"@microsoft.graph.downloadUrl\"]\n",
        "        content = requests.get(download_url).content\n",
        "        # Load all sheets in the Excel file as a dict of DataFrames\n",
        "        all_sheets = pd.read_excel(io.BytesIO(content), sheet_name=None)\n",
        "        res = requests.patch(graph_patch, headers=headers,json={\n",
        "            \"parentReference\": {\n",
        "            \"id\": \"01ZR6EH4FUACIO2UTSDRE2XAEM5WSYST2V\"\n",
        "            },\n",
        "            \"name\": f[\"name\"]\n",
        "        })\n",
        "\n",
        "\n",
        "        # Iterate over each sheet's data frame\n",
        "        for sheet_name, df in all_sheets.items():\n",
        "            if sheet_name == 'Header':\n",
        "                header_table = Table('transactions', metadata,schema = \"public\",autoload_with=engine)\n",
        "                table_columns = {col.name for col in header_table.columns}\n",
        "                # Perform upserts\n",
        "                with engine.begin() as conn:\n",
        "                    for row in df.to_dict(orient='records'):\n",
        "                        filtered_row = {k.strip(): v for k, v in row.items() if k.strip() in table_columns}\n",
        "                        stmt = insert(header_table).values(**filtered_row)\n",
        "                        # Replace 'id' with your unique key / primary key column\n",
        "                        upsert_stmt = stmt.on_conflict_do_update(\n",
        "                            index_elements=['transaction_id'],  # unique key column(s)\n",
        "                            set_={c.name: stmt.excluded[c.name] for c in header_table.columns if c.name != 'transaction_id'}\n",
        "                        )\n",
        "                        conn.execute(upsert_stmt)\n",
        "                #     spark_df = spark.createDataFrame(df.to_dict(orient=\"records\"))\n",
        "            #     spark_df = spark_df.withColumn(\"Transaction Date\", to_date(\"Transaction Date\", \"yyyy-MM-dd\"))\n",
        "            #     spark_df = spark_df.withColumn(\"Transaction Id\", col(\"Transaction Id\").cast(\"string\"))\n",
        "            #     spark_df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(\"append\").saveAsTable(\"databricks_ws_2860374985950859.default.header\")\n",
        "            elif sheet_name == 'Items' :\n",
        "                item_table = Table('transaction_item', metadata, schema='public',autoload_with=engine)\n",
        "                table_columns = {col.name for col in item_table.columns}\n",
        "                                # Perform upserts\n",
        "                with engine.begin() as conn:\n",
        "                    for row in df.to_dict(orient='records'):\n",
        "                        filtered_row = {k: v for k, v in row.items() if k in table_columns}\n",
        "                        stmt = insert(item_table).values(**filtered_row)  # Changed header_table to item_table\n",
        "                        # Use both 'transaction_id' and 'item_id' as unique key columns for upsert\n",
        "                        upsert_stmt = stmt.on_conflict_do_update(\n",
        "                            index_elements=['transaction_id', 'item_id'],  # unique key columns\n",
        "                            set_={c.name: stmt.excluded[c.name] for c in item_table.columns if c.name not in ['transaction_id', 'item_id']}\n",
        "                        )\n",
        "                        conn.execute(upsert_stmt)\n",
        "            #     spark_df = spark.createDataFrame(df.to_dict(orient=\"records\"))\n",
        "            #     spark_df = spark_df.withColumn(\"Service Rendered Date\", to_date(\"Service Rendered Date\", \"yyyy-MM-dd\"))\n",
        "            #     spark_df = spark_df.withColumn(\"Transaction Id\", col(\"Transaction Id\").cast(\"string\"))\n",
        "            #     spark_df = spark_df.withColumn(\"Item Id\", col(\"Item Id\").cast(\"string\"))\n",
        "            #     spark_df.write.mode(\"append\").saveAsTable(\"databricks_ws_2860374985950859.default.item\")\n",
        "            combined_data.extend(df.to_dict(orient=\"records\"))\n",
        "            print(f\"✅ Fetched and combined: {f['name']} - {sheet_name} ({len(df)} rows)\")\n",
        "    # Save one combined JSON file\n",
        "    combined_json_path = os.path.join(output_dir, f[\"name\"].replace(\".xlsx\", \".json\"))\n",
        "    with open(combined_json_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "            json.dump(combined_data, f_out, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n✅ All Excel files merged → {combined_json_path}\")\n",
        "    print(f\"📊 Total records combined: {len(combined_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiDzGK7BQyEF",
        "outputId": "c0e36421-823d-4036-93b4-3bfa90e45262"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fetched and combined: Transaction_0137413413.xlsx - Header (49 rows)\n",
            "✅ Fetched and combined: Transaction_0137413413.xlsx - Items (19 rows)\n",
            "\n",
            "✅ All Excel files merged → /Workspace/processed/Transaction_0137413413.json\n",
            "📊 Total records combined: 68\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}